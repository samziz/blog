# Devising a new compression algorithm

We can break existing compression algorithms down into a few different 'tricks' used to express N bits of information in N/L bits, where L is some large number. These tricks are:

## Interesting properties of data

These properties of data help or hinder compression. Some may only be relevant when using certain techniques to compress data.

### Kolmogorov density

The **Kolmogorov complexity** of a chunk of data represents the complexity of the (least complex, hypothetical) program required to produce it. For instance, the random sequence `[1, 37, 6, 5, 7, 2, 89, 5, 4, 23]` has high Kolmogorov complexity, because the shortest program is probably `return [1, 37, 6, 5, 7, 2, 89, 5, 4, 23]`. The equally long sequence `[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]`, namely the Fibonacci numbers, has a lower Kolmogorov complexity. We can express it as something like `start with [0, 1]; loop n times, adding the numbers at indices len-(n+1) and len-(n+2) each time` (the complexity is not about the string length of the instructions, but the abstract 'amount of information' they contain).

This covers Kolmogorov complexity. Now, the interesting property - from the point of view of assessing compressibility of a chunk of data - is not _absolute_ Kolmogorov complexity, but a measure which for want of a better word we could call **Kolmogorov density**. That is: the relation between the Kolmogorov complexity of a chunk of data, and its length. This means that a random sequence of 10 elements has equal Kolmogorov density as another random sequence of 100 elements â€“ it's all about the proportion of the data which is expressible in the form of instructions more concise than that data itself.

## Compression techniques

- **Keying repeated sequences**: Any sequence of bits which is repeated at least once can be replaced with a very short 'key'. All keys are then stored in a simple table along with the extended sequences they represent. This is most effective when there are
